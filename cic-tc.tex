% 
% exemplo genérico de uso da classe iiufrgs.cls
% $Id: iiufrgs.tex,v 1.1.1.1 2005/01/18 23:54:42 avila Exp $
% 
% This is an example file and is hereby explicitly put in the
% public domain.
% 
\documentclass[cic,tc]{iiufrgs}
% Para usar o modelo, deve-se informar o programa e o tipo de documento.
% Programas :
% * cic       -- Graduação em Ciência da Computação
% * ecp       -- Graduação em Ciência da Computação
% * ppgc      -- Programa de Pós Graduação em Computação
% * pgmigro   -- Programa de Pós Graduação em Microeletrônica
% 
% Tipos de Documento:
% * tc                -- Trabalhos de Conclusão (apenas cic e ecp)
% * diss ou mestrado  -- Dissertações de Mestrado (ppgc e pgmicro)
% * tese ou doutorado -- Teses de Doutorado (ppgc e pgmicro)
% * ti                -- Trabalho Individual (ppgc e pgmicro)
% 
% Outras Opções:
% * english    -- para textos em inglês
% * openright  -- Força início de capítulos em páginas ímpares (padrão da
% biblioteca)
% * oneside    -- Desliga frente-e-verso
% * nominatalocal -- Lê os dados da nominata do arquivo nominatalocal.def


% Use unicode
\usepackage[utf8]{inputenc}   % pacote para acentuação

% Necessário para incluir figuras
\usepackage{graphicx}         % pacote para importar figuras

\usepackage{times}            % pacote para usar fonte Adobe Times
% \usepackage{palatino}
% \usepackage{mathptmx}       % p/ usar fonte Adobe Times nas fórmulas
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amsthm}      % Teoremas
\usepackage{thmtools}    % Front end para amsthm (\declaretheorem)

\declaretheorem[style=definition,name=Definição,parent=chapter,qed=\textemdash]{definicao}
\declaretheorem[style=plain,name=Teorema,qed=\textnormal{\textemdash}]{teorema}
\declaretheorem[style=plain,name=Axioma,qed=\textnormal{\textemdash}]{axioma}

\usepackage[alf,abnt-emphasize=bf]{abntex2cite}	% pacote para usar citações abnt

\usepackage[portuguese,ruled,vlined,boxed]{algorithm2e}
\renewcommand{\algorithmautorefname}{Algoritmo}% para utilizar \autoref

\def\SPSB#1#2{\rlap{\textsuperscript{#1}}\SB{#2}}
\def\SP#1{\textsuperscript{#1}}
\def\SB#1{\textsubscript{#1}}

\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\mat}[1]{\bm{#1}}

% 
% Informações gerais
% 
\title{Utilização de compressed sensing para compressão de frames e transmissão de vídeos em tempo real.}

\author{Sachser}{Eduardo}
% alguns documentos podem ter varios autores:
% \author{Flaumann}{Frida Gutenberg}
% \author{Flaumann}{Klaus Gutenberg}

% orientador e co-orientador são opcionais (não diga isso pra eles :))
\advisor[Prof.~Dr.]{Oliveira Neto}{Manuel Menezes de }
% \coadvisor[Prof.~Dr.]{Knuth}{Donald Ervin}

% a data deve ser a da defesa; se nao especificada, são gerados
% mes e ano correntes
% \date{maio}{2001}

% o local de realização do trabalho pode ser especificado (ex. para TCs)
% com o comando \location:
% \location{Itaquaquecetuba}{SP}

% itens individuais da nominata podem ser redefinidos com os comandos
% abaixo:
% \renewcommand{\nominataReit}{Prof\textsuperscript{a}.~Wrana Maria Panizzi}
% \renewcommand{\nominataReitname}{Reitora}
% \renewcommand{\nominataPRE}{Prof.~Jos{\'e} Carlos Ferraz Hennemann}
% \renewcommand{\nominataPREname}{Pr{\'o}-Reitor de Ensino}
% \renewcommand{\nominataPRAPG}{Prof\textsuperscript{a}.~Joc{\'e}lia Grazia}
% \renewcommand{\nominataPRAPGname}{Pr{\'o}-Reitora Adjunta de P{\'o}s-Gradua{\c{c}}{\~a}o}
% \renewcommand{\nominataDir}{Prof.~Philippe Olivier Alexandre Navaux}
% \renewcommand{\nominataDirname}{Diretor do Instituto de Inform{\'a}tica}
\renewcommand{\nominataCoord}{Prof.~Sérgio Luis Cechin}
% \renewcommand{\nominataCoordname}{Coordenador do PPGC}
% \renewcommand{\nominataBibchefe}{Beatriz Regina Bastos Haro}
% \renewcommand{\nominataBibchefename}{Bibliotec{\'a}ria-chefe do Instituto de Inform{\'a}tica}
% \renewcommand{\nominataChefeINA}{Prof.~Jos{\'e} Valdeni de Lima}
% \renewcommand{\nominataChefeINAname}{Chefe do \deptINA}
% \renewcommand{\nominataChefeINT}{Prof.~Leila Ribeiro}
% \renewcommand{\nominataChefeINTname}{Chefe do \deptINT}

% A seguir são apresentados comandos específicos para alguns
% tipos de documentos.

% Relatório de Pesquisa [rp]:
% \rp{123}             % numero do rp
% \financ{CNPq, CAPES} % orgaos financiadores

% Trabalho Individual [ti]:
% \ti{123}     % numero do TI
% \ti[II]{456} % no caso de ser o segundo TI

% Monografias de Especialização [espec]:
% \espec{Redes e Sistemas Distribuídos}      % nome do curso
% \coord[Profa.~Dra.]{Weber}{Taisy da Silva} % coordenador do curso
% \dept{INA}                                 % departamento relacionado

% 
% palavras-chave
% iniciar todas com letras minúsculas, exceto no caso de abreviaturas
% 
\keyword{Compressed Sensing}
\keyword{Compressed Sensing}
\keyword{Compressive Sampling}
\keyword{Real Time}
\keyword{Video}
\keyword{UFRGS}

%\settowidth{\seclen}{1.10~}

% 
% inicio do documento
% 
\begin{document}

% folha de rosto
% às vezes é necessário redefinir algum comando logo antes de produzir
% a folha de rosto:
% \renewcommand{\coordname}{Coordenadora do Curso}
\maketitle

% dedicatoria
% \clearpage
% \begin{flushright}
%     \mbox{}\vfill
%     {\sffamily\itshape
%       ``If I have seen farther than others,\\
%       it is because I stood on the shoulders of giants.''\\}
%     --- \textsc{Sir~Isaac Newton}
% \end{flushright}

% agradecimentos
%\chapter*{Agradecimentos}
%Agradeço ao \LaTeX\ por não ter vírus de macro\ldots



% resumo na língua do documento
\begin{abstract}
    Deve ser feito ainda.
\end{abstract}

% resumo na outra língua
% como parametros devem ser passados o titulo e as palavras-chave
% na outra língua, separadas por vírgulas
\begin{englishabstract}{Translate title to english.}{Compressed Sensing. Compressive Sensing. UFRGS}
    Must be done yet.
\end{englishabstract}

% lista de figuras
\listoffigures

% lista de tabelas
\listoftables

% lista de abreviaturas e siglas
% o parametro deve ser a abreviatura mais longa
\begin{listofabbrv}{CoSaMP}
    \item[RIP] Restricted Isometric Property
    \item[OMP] Orthogonal Matching Pursuit
    \item[CoSaMP] Compressive Sampling Matching Pursuit 
\end{listofabbrv}

% idem para a lista de símbolos
\begin{listofsymbols}{$\alpha\beta\pi\omega$}
    \item[$\sum{\frac{a}{b}}$] Somatório do produtório
    \item[$\alpha\beta\pi\omega$] Fator de inconstância do resultado
\end{listofsymbols}

% sumario
\tableofcontents

% aqui comeca o texto propriamente dito

% introducao
\chapter{Introdução}
No início dos tempos, Donald E. Knuth criou o \TeX. Algum tempo depois, Leslie Lamport criou o \LaTeX. Graças a eles, não somos obrigados a usar o Word nem o LibreOffice.

\section{Figuras e tabelas}

\begin{figure}[h]
    \caption{Descrição da Figura deve ir no topo}
    \begin{center}
        % Aqui vai um includegraphics , um picture environment ou qualquer
        % outro comando necessário para incorporar o formato de imagem
        % utilizado.        
        \begin{picture}(100,100)
            \put(0,0){\line(0,1){100}}
            \put(0,0){\line(1,0){100}}
            \put(100,100){\line(0,-1){100}}
            \put(100,100){\line(-1,0){100}}
            \put(10,50){Uma Imagem}
        \end{picture}    
    \end{center}
    \label{fig:estrutura}
    \legend{Fonte: Os Autores}
\end{figure}


% \begin{figure}
%     \caption{Exemplo de figura importada de um arquivo e também exemplo de caption muito grande que ocupa mais de uma linha na Lista~de~Figuras}
%     \begin{center}
%         \includegraphics[width=8em]{fig}
%     \end{center}
%     \legend{Fonte: Os Autores}
%     \label{fig:ex1}
% \end{figure}

% o `[h]' abaixo é um parâmetro opcional que sugere que o LaTeX coloque a
% figura exatamente neste ponto do texto. Somente preocupe-se com esse tipo
% de formatação quando o texto estiver completamente pronto (uma frase a mais
% pode fazer o LaTeX mudar completamente de idéia sobre onde colocar as
% figuras e tabelas)
% \begin{figure}[h]
\begin{figure}
    \caption{Exemplo de figura desenhada com o environment \texttt{picture}.}
    \begin{center}
        \setlength{\unitlength}{.1em}
        \begin{picture}(100,100)
            \put(20,20){\circle{20}}
            \put(20,20){\small\makebox(0,0){a}}
            \put(80,80){\circle{20}}
            \put(80,80){\small\makebox(0,0){b}}
            \put(28,28){\vector(1,1){44}}
        \end{picture}
    \end{center}
    \legend{Fonte: Os Autores}
    \label{fig:ex2}
\end{figure}

Tabelas são construídas com praticamente os mesmos comandos. Ver a tabela \ref{tbl:ex1}.

\begin{table}[h]
    \caption{Uma tabela de Exemplo}
    % OBS: não use \begin{center}, pois este aumenta o espaçamento entre a caption/legend e a tabela
    % Para figuras, a aparência é melhor com o espaçamento extra
    \centering
        \begin{tabular}{c|c|p{5cm}}
          \hline
          \textit{Col 1}  &   \textit{Col 2}  &   \textit{Col 3} \\
          \hline
          \hline
          Val 1           &   Val 2           & Esta coluna funciona como um parágrafo, tendo uma margem definida em 5cm. Quebras de linha funcionam como em qualquer parágrafo do tex. \\
          Valor Longo     & Val 2             & Val 3 \\
          \hline
        \end{tabular}
    \legend{Fonte: Os Autores}
    \label{tbl:ex1}
\end{table}

\subsection{Formato de Figuras}
\label{sec:fig_format}

O LaTeX permite utilizar vários formatos de figuras, entre eles \emph{eps}, \emph{pdf}, \emph{jpeg} e \emph{png}. Programas de diagramação como Inkscape (e mesmo LibreOffice) permitem gerar arquivos de imagens vetoriais que podem ser utilizados pelo LaTeX sem dificuldade. Pacotes externos permitem utilizar SVG e outros formatos.

Dia e xfig são programas utilizados por dinossauros para gerar figuras vetoriais. Se possível, evite-os.

\subsection{Classificação dos etc.}

O formato do instituo de informática define 5 níveis: capítulo, seção, subseção e outros 2 sem nome.

\subsubsection{Subsubseção}
Exemplo de uma subsubseção.

\paragraph{Parágrafo}
Exemplo de um parágrafo.

\section{Sobre as referências bibliográficas}

A classe \emph{iiufrgs} faz uso do pacote \emph{abnTeX2} com algumas alterações
feitas por Sandro Rama Fiorini. Culpe ele se algo der errado. Agradeça a ele
pelo que der certo. As modificações dão uma camada de tinta NATBIB-style,
já que o abntex2 usa uns comandos de citação feitos para alienígenas de 5 braços
wtf. Exemplos de citação:

\begin{itemize}
    \item \emph{cite}: Unicórnios são verdes \cite{WirelessXiangCai};
    \item \emph{citep}:Unicórnios são verdes \citep{WirelessXiangCai};
    \item \emph{citet}: Segundo \citet{WirelessXiangCai}, unicórnios são
    verdes.
    \item \emph{citen or citenum}: Segundo \citen{WirelessXiangCai},
    unicórnios são verdes.
    \item \emph{citeauthor e citeyearpar}: Segundo artigos de
    \citeauthor{WirelessXiangCai} , unicórnios são verdes
    \citeyearpar{WirelessXiangCai}.

\end{itemize}

O estilo abnt fornecido antigamente pelo UTUG não é mais recomendado, pois não
produz saída de acordo com as exigências da biblioteca.

Recomenda-se o uso de bibtex para gerenciar as referências (veja o arquivo
biblio.bib).


\chapter{Referencial Teórico}

\section{Representação de Sinais}

\section{Compressed Sensing}
\subsection{O modelo de Compressed Sensing}
\textit{Compressed Sensing (CS)} ou \textit{Compressive Sampling} é um campo de estudo de sinais cujos
primeiros trabalhos surgiram a partir de 2006, a partir da análise de amostragens executadas em imagens de 
ressonância magnética. Essas amostragens eram de muito menor dimensionalidade do que a da imagem, o que 
fez com que os métodos conhecidos de processamento de sinais não tivessem boa acurácia.

"Muitos sinais de interesse possuem menos informação do que a dimensão do ambiente sugere"
\footnote{"Many signals of interest contain far less information than their ambient dimension suggests"}
(\citeauthor{chen2015compressed}, \citeyear{chen2015compressed}, p. 2, tradução própria).
As formas tradicionais de aquisição de sinais baseadas nos teoremas de Shannon e Nyquist acabam por causar o descarte
de boa parte da informação coletada durante a etapa de compressão. Tal fato traz a tona a pergunta: é possível haver 
uma forma de aquisição dos sinais na qual as amostras comprimidas são obtidas diretamente? 

O modelo de Compressed Sensing surgiu com esse intuito \cite{DonohoCS}. Trabalhos na área mostram que, para certas classes
de sinais, poucas amostras são necessárias para representar o sinal com acurácia \cite{chen2015compressed}.
Ou seja, Compressed Sensing busca encontrar quais tipos
de sinais podem ser amostrados de forma já comprimida e respostas de como fazê-lo.

De acordo com o modelo, um sinal $ \vec{f} $ em geral é um elemento de $ \mathbb{C}^d $. Amostragens são executadas na forma:
\begin{equation}
    y_i = \langle \vec{\phi_i}, \vec{f} \rangle \text{ para } i=1,2,...,m, 
\end{equation}
na qual $m \ll d$. Os vetores $\vec{\phi_i}$ são linhas de uma matriz $m \times d$ $\mathbf{\Phi}$, chamada matriz 
de amostragem.Assim, podemos reescrever o vetor de amostragem como $\vec{y} = \mathbf{\Phi} \vec{f}$. Fica claro que, 
se $ m \ll d$, reconstruir $\vec{f}$ a partir de $\vec{y}$, sem assumir nada a mais, é um problema 
com infinitas soluções \cite{chen2015compressed}.

Uma importante consideração que CS faz é a de que os sinais de interesse possuem menos informação do que sugere a 
dimensão $d$. Uma forma de quantificar essa noção é a esparsidade, que pode ser definida como a quantidade de valores não
nulos $s$ de um sinal $\vec{f} \in \mathbb{C}^d$, também conhecida como norma $\ell_0$:
\begin{equation}
    \lVert \vec{f} \rVert_0 = s
\end{equation}

Considerando o mesmo sinal $\vec{f}$, diz-se que este é \textit{s-esparso}, quando, para dado um um valor $s$, $\vec{f}$
satisfaz a seguinte inequação:
\begin{equation}
    \label{eq:f0less}
    \lVert \vec{f} \rVert_0 \le s \ll d
\end{equation} 

Na prática, sinais não são usualmente encontrados esparsos, o que leva a definição de que \textit{sinais compressíveis}
são aqueles que obedecem a seguinte lei de decaimento exponencial:
\begin{equation}
    | f\SPSB{*}{k} | < R k^{-\frac{1}{q}},  
\end{equation}
tal que $\vec{f}^* $ é o rearranjo decrescente de $\vec{f}$, $R$ é uma constante positiva e $0< q < 1$. Percebe-se
que para valores bem pequenos de $q$ a compressibilidade se torna praticamente o mesmo que esparsidade. Se considerarmos
$\vec{f_s}$ o vetor com os $s$ maiores valores de $\vec{f}$ em magnitude, temos que, para sinais compressíveis $\vec{f}$ e 
$\vec{f_s}$:
\begin{equation}
    \lVert \vec{f} - \vec{f_s} \rVert_2 \le Rs^{\frac{1}{2} - \frac{1}{q}} \hspace{1em} \text{ e } \hspace{1em}
    \lVert \vec{f} - \vec{f_s} \rVert_1 \le Rs^{1 - \frac{1}{q}} 
\end{equation} 

Por fim, a definição \eqref{eq:f0less} exige que $\vec{f}$ seja esparso, ou seja, tenha $s$ coeficientes não nulos
no máximo. Por outro lado, $\vec{f}$ pode ser esparso em alguma outra base ortonormal $\mathbf{D}$, uma \textit{sparsifying basis} 
\cite{CandesDecoLinear}. Nesse caso, considera-se $\vec{f}$ s-esparso se:
\begin{equation}
    \vec{f} = \mathbf{D}\vec{x} \hspace{1em} \text{dado que} \hspace{1em} \lVert \vec{x} \rVert_0 \le s \ll d.
\end{equation}


\subsection{Mecanismos de Amostragem}
A partir das definições básicas do modelo, podemos formular o problema básico de CS da seguinte forma:

Tomando um operador de amostragem $\mathbf{\Phi}$, mapeamento linear de $\mathbb{C}^d$ para algum espaço de dimensão 
$\mathbb{C}^m$, para recuperarmos um sinal $\vec{f}$ a partir de suas medidas $\vec{y} = \mathbf{\Phi} \vec{f}$, podemos escrever
como um problema de minimização.
\begin{equation}
    \label{eq:problem}
    \vec{f}' = \underset{\vec{g} \in \mathbb{C}^d}{\text{argmin}} \lVert \vec{g} \rVert_0 \hspace{1em} \text{sujeito a} \hspace{1em}
    \mathbf{\Phi} \vec{g} = \vec{y} 
\end{equation}

A partir da formulação, se $\mathbf{\Phi}$ não mapeia 2 vetores esparsos quaisquer para a mesma imagem, ou seja,
se $\vec{f} \ne \vec{g} \rightarrow \mathbf{\Phi}\vec{f} \ne \mathbf{\Phi}\vec{g} $, então a solução $\vec{f}'$ recupera
$\vec{f}$, $\vec{f}' = \vec{f}$ \cite{chen2015compressed}. Esse problema, por outro lado, é intratável, e, em geral,
NP-difícil \cite{Mut05}.
Para que o operador de amostragem forneça o resultado anterior, ele deve ser \textit{incoerente}.
Dado um operador $\mathbf{\Phi}$ de colunas $\{ \vec{\phi_i} \}$  com norma unitária, define-se a sua coerência $\mu$
como a maior correlação entre suas colunas.
\begin{equation}
    \label{eq:coerence}
    \mu = \underset{i \ne j}{max}\langle \vec{\phi_i} , \vec{\phi_j} \rangle
\end{equation}

Fica definido, portanto, que um operador de amostragem é \textit{incoerente} quando a sua coerência $\mu$ é 
suficientemente pequena. Por exemplo, um operador de amostragem que seja uma base ortonormal é incoerente, bem como
operadores que sejam aproximadamente ortonormais para vetores esparsos.

Uma propriedade que captura a mesma ideia que o referido acima foi desenvolvida por Candès e Tao, chamada 
\textit{Restricted Isometry Property}(RIP)\cite{CandesSignalRecovery}. A constante de isometria restrita 
$\delta_s$ é a menor tal que:
\begin{equation}
    \label{eq:rip}
    (1 - \delta_s)\lVert \vec{f} \rVert\SPSB{2}{2} \le \lVert \mathbf{\Phi} \vec{f} \rVert \SPSB{2}{2} \le 
    (1 + \delta_s)\lVert \vec{f} \rVert\SPSB{2}{2} \hspace{1em} \text{para todo vetor s-esparso } \vec{f}.
\end{equation}

Diz-se que um operador de amostragem $\mathbf{\Phi}$ tem o RIP de ordem $s$ quando $\delta_s$ é suficientemente
pequeno, por exemplo, $\delta_s \le 0.1$.

A questão importante da RIP é descobrir qual o número de amostras $m$ é necessária e quais são as 
classes de matrizes que possuem a propriedade. Duas classes de matrizes que possuem a RIP são as 
matrizes subgaussianas \cite{Mendelson2008} e as matrizes ortogonais parcialmente limitadas \cite{rudelson2008sparse}.
\textbf{Preciso falar sobre essas matrizes mais a fundo??????????????}


\subsection{Algoritmos aplicados ao modelo de Compressive Sensing}
Ao considerar-se a formulação do problema geral de CS apresentada na seção anterior, um dos resultados também apresentados
é de que o problema é NP-difícil. Sob essa ótica, diversas soluções alternativas foram desenvolvidas com 
o intuito de, através da solução de algum outro problema relacionado, ou do uso de alguma diferente estratégia, fosse possível 
chegar a uma solução ótima para a formulação de CS. 

Inicialmente, é necessário levar em consideração que nem sempre as amostras coletadas estão completamente corretas,
o que leva a um novo vetor de medidas, $\vec{y} = \mathbf{\Phi} \vec{f} + \vec{e}$, que considera o ruído de amostragem, 
ou vetor de erro, $\vec{e}$.

Além disso, é essencial definir as propriedades ideais de um método de recuperação
baseado em CS. Segundo \citet{chen2015compressed}, as propriedades são as seguintes:
\begin{itemize}
    \item \textbf{Amostragem não adaptativa:} Os operadoes de amostragem não devem ser dependentes do sinal. 
          Operadores que possuem RIP também possuem essa propriedade.
    \item \textbf{Número ótimo de amostras:} O número de amostras necessárias deve ser mínimo.
    \item \textbf{Garatia de Uniformidade:} Um único operador de amostragem deve ser suficiente para qualquer sinal.
    \item \textbf{Robustez:} O método deve ser estável e robusto em relação ao ruído, e possuir garantias 
          quanto ao erro.
    \item \textbf{Complexidade:} O algoritmo deve ser eficiente.
\end{itemize}

Buscando obedecer o maior número das propriedades citadas acima, alguns métodos e algoritmos surgiram. 
Nas subseções seguintes serão descritas duas metodologias que foram utilizadas para tal.

\subsubsection{Métodos baseados em otimização}
Essa metodologia, utilizada pelos primeiros trabalhos na área, utiliza uma relaxação convexa como forma
de chegar a solução do problema \eqref{eq:problem}. Ou seja, utiliza a norma $\ell_1$ ao invés da norma $\ell_0$,
de forma que o problema se torna convexo e solúvel através de métodos de programação linear.

Dessa forma, o problema relaxado para a norma $\ell_1$ fica como segue:
\begin{equation}
    \label{eq:probleml1}
    \vec{f'} = \underset{\vec{g} \in \mathbb{C}^d}{\text{argmin}} \lVert \vec{g} \rVert_1 \hspace{1em} \text{sujeito a} \hspace{1em}
    \lVert \mathbf{\Phi} \vec{g} - \vec{y} \rVert_2 \le \epsilon
\end{equation}
tal que $\lVert \vec{e} \rVert_2 \le \epsilon $.

A geometria da norma $\ell_1$ permite esparsidade, como pode-se ver no exemplo a seguir. Imaginando-se 
que a reta em verde retrata todas as possíveis soluções para um sinal $\vec{f} \in \mathbb{R}^2$, os pontos
mais esparsos da reta são $A=(0;1)$ e $B=(-1.\overline{6};0)$. A região em azul limita todos os pontos 
nos quais $\lVert\vec{p}\rVert_1 \le 1.0$, tal que $\vec{p} \in \mathbb{R}^2$. Nesse caso, o ponto $A$ é o 
de menor norma $\ell_1$ da reta, sendo essa a solução do problema \eqref{eq:probleml1} e, ao mesmo tempo, 
uma das possíveis soluções do problema \eqref{eq:problem}.
\begin{figure}[h]
    \caption{Minimização $\ell_1$ nos pontos da reta.}
    \begin{center}
        \includegraphics[width=0.9\textwidth]{img//l1ball}
    \end{center}
    \legend{Fonte: O Autor}
    \label{fig:l1ball}
\end{figure}

Candès, Romberg e Tao chegaram a resultados quanto a garantias para os limites de erro, baseado na 
intensidade do ruído, como segue.
\begin{teorema}
    \cite{candes2006stable}. 
    Dado um operador de amostragem $\mathbf{\Phi}$ que satisfaz a RIP.
    Então, para qualquer sinal $\vec{f}$ e sua amostragem ruidosa $\vec{y} = \mathbf{\Phi}\vec{f} + \vec{e}$, 
    tal que $\lVert \vec{e} \rVert_2 \le \epsilon$, a solução $\vec{f}'$ de \eqref{eq:probleml1} satisfaz:
    \begin{equation*}
        \lVert \vec{f}' - \vec{f} \rVert_2 \le C \left[ \epsilon + \frac{\lVert \vec{f} - \vec{f_s} \rVert_1}{\sqrt{s}} \right],
    \end{equation*}
    tal que $\vec{f_s}$ denota o vetor com os $s$ maiores coeficientes em magnitude de $\vec{f}$ e $C$ é uma
    constante tal que $C \ge 0$.
\end{teorema}

\subsubsection{Métodos utilizando algoritmos gulosos}
Depois da formalização do problema de CS, unido ao estouro da novo assunto que se apresentou,
surgiram os primeiros algoritmos que, de forma iterativa, resolvem o problema \eqref{eq:problem}. 
Um dos principais algoritmos é o \textit{Orthogonal Matching Pursuit (OMP)}, analisado por Gilbert e Tropp \cite{GilbertOMP}.

Segundo \citet{GilbertOMP}, o algoritmo do OMP é o que segue:

\begin{algorithm}[H]
\caption{\textit{Orthogonal Matching Pursuit (OMP)}}
\Entrada{Matriz de medidas $m \times d$ $\mathbf{\Phi}$, Vetor de $m$ medidas $\vec{y}$, nível de esparsidade $s$}
\Saida{Estimativa do sinal ideal $\vec{f'} \in \mathbb{R}^d$}
$\vec{r^0} \leftarrow \vec{y}$\;
$t \leftarrow 1$\;
$\Lambda^0 \leftarrow ()$\;
$\vec{f'} \leftarrow \vec{0}$\;
\While{$t \le s$}{
    $\lambda^t = \underset{j=1,...,d}{argmax}|\langle\vec{r^{t-1}}, \vec{\phi_j}\rangle|$\;
    $\Lambda^t = \Lambda^{t-1} \cup \left\{\lambda^t\right\}$\;
    $\mathbf{\Phi^t} = \left[ \mathbf{\Phi^{t-1}} \hspace{0.5em} \vec{\phi_{\lambda^t}} \right]$\;
    Resolver problema de mínimos quadrados: $\vec{x^t} = \underset{\vec{x}}{argmin} \lVert \vec{y} - \mathbf{\Phi^t} \vec{x} \rVert^2$\;
    $\vec{a^t} \leftarrow \mathbf{\Phi^t} \vec{x^t}$\;
    $\vec{r^t} \leftarrow \vec{y} - \vec{a^t}$\;
    $t \leftarrow t + 1$\;
}
$\vec{f'}_{\Lambda^{t-1}} \leftarrow \vec{x^{t-1}}$\;
\end{algorithm}

Uma importante observação relativa ao sucesso do algoritmo OMP é que, dado que a matriz $\mathbf{\Phi}$ é incoerente,
seu conjugado transposto multiplicado por si mesmo é próximo a identidade, ou seja, 
$\vec{u} = \mathbf{\Phi}^* \vec{y} = \mathbf{\Phi}^* \mathbf{\Phi} \vec{f}$ 
é, de certa forma, muito próximo a $\vec{f}$. Logo, o OMP considera que o mais alto coeficiente de 
$\vec{u}$ é um dos coeficientes esparsos de $\vec{f}$ \cite{chen2015compressed}.

Gilbert e Tropp ainda apresentam um teorema sobre o algoritmo.
\begin{teorema}
    \cite{GilbertOMP}.
    Dado $\mathbf{\Phi}$ uma matriz $m \times d$ subgaussiana de medidas tal que $m \ge C s log d$ e
    $\vec{f}$ um sinal s-esparso em $\mathbb{R}^d$.
    Então, com alta probabilidade, OMP reconstrui corretamente o sinal $\vec{f}$ a partir de suas
    medidas $\vec{y} = \mathbf{\Phi}\vec{f}$.
\end{teorema}
Sem nenhuma modificação, o OMP não é reconhecidamente robusto a ruído, nem oferece garantias de uniformidade.
Porém, esse algoritmo possui um baixo custo computacional e boa eficiência, tendo complexidade de
$O(s m d)$ \cite{chen2015compressed}. 
Algo que fica claro, observando-se o pseudo-código e a complexidade, é que o OMP tem desempenho diretamente
proporcional com a esparsidade da solução, ou seja, a cada nova iteração no \textit{loop} principal, um 
novo coeficiente da solução é calculado.

Outros algoritmos de estratégia gulosa oferecem garantias de uniformidade, além de robustez a ruído, com 
limites demonstrados, bem como número de iterações e medidas. Dois principais destes algoritmos são o 
\textit{CoSaMP (Compressive Sampling Matching Pursuit)} \cite{NeedellCoSaMP}, e o 
\textit{IHT (Iterative Hard Thresholding)} \cite{BLUMENSATHIHT}.

\subsection{Resultados com dicionários coerentes}
A utilização de matrizes de amostragens incoerentes, ou dicionários incoerentes, limita a aplicação de 
Compressed Sensing em algumas áreas, ou para alguns problemas cuja base de solução dos mesmos era baseada
em dicionários coerentes, que podem ser, por exemplo, um dicionário supercompleto, com mais colunas que linhas. 
"Coerência é, de certa forma, uma propriedade natural para Compressed Sensing, 
porque se duas colunas são muito correlatas, será impossível em geral distinguir se a energia do sinal
vem de uma ou da outra" 
\footnote{"Coherence is in some sense a natural property in the compressed sensing framework, for if two
columns are closely correlated, it will be impossible in general to distinguish whether the energy in 
the signal comes from one or the other"}
(\citeauthor{CANDESDICTS}, \citeyear{CANDESDICTS}, tradução própria).

Porém, em alguns casos, não precisa-se recuperar os coeficientes originais do sinal, o que com dicionários 
coerentes não é possível, mas sim o próprio sinal já transportado para uma nova base.
Matematicamente falando, imagine-se um sinal $\vec{f} = \mathbf{D}\vec{x}$, no qual suas medidas são
$\vec{y} = \mathbf{\Phi}\vec{f} = \mathbf{\Phi}\mathbf{D}\vec{x}$, se o dicionário $\mathbf{D}$ for
coerente, não é possível recuperar os coeficientes $\vec{x}$, mas é possível recuperar $\vec{f}$ \cite{CANDESDICTS}.

A partir desses estudos, \citet{CANDESDICTS} gerou uma nova definição para o problema de CS, 
baseado na solução utilizando a norma $\ell_1$, considerando $\vec{f} = \mathbf{D}\vec{x}$ o sinal
e $\vec{y} = \mathbf{\Phi}\vec{f} + \vec{e}$ as medidas, $\vec{e}$ o erro.
\begin{equation}
    \label{eq:problemDl1}
    \vec{f'} = \underset{\vec{g} \in \mathbb{C}^d}{\text{argmin}} \lVert \mathbf{D}^*\vec{g} \rVert_1 
    \hspace{1em} \text{sujeito a} \hspace{1em}
    \lVert \mathbf{\Phi} \vec{g} - \vec{y} \rVert_2 \le \epsilon
\end{equation}
tal que $\lVert \vec{e} \rVert_2 \le \epsilon $.

\citet{CANDESDICTS} também redefiniu a RIP, chamando de D-RIP como segue. 
Dado um operador de amostragem $\mathbf{\Phi}$, ele satisfaz o D-RIP para um determinado 
dicionário $\mathbf{D}$ de ordem $s$ se:
\begin{equation}
    (1 - \delta_s)\lVert \mathbf{D}\vec{x} \rVert\SPSB{2}{2} \le \lVert \mathbf{\Phi} \mathbf{D}\vec{x} \rVert \SPSB{2}{2} \le 
    (1 + \delta_s)\lVert \mathbf{D}\vec{x} \rVert\SPSB{2}{2} \hspace{1em} \text{para todo vetor s-esparso } \vec{x}.
\end{equation}
para algum $\delta_s$, por exemplo $\delta_s \le 0.08$.

Tomando a nova definição, \citet{CANDESDICTS} provou o seguinte teorema sobre a recuperação do sinal.
\begin{teorema}
    \cite{CANDESDICTS}
    Dado $\mathbf{D}$ um dicionário e supondo um operador de amostragem $\mathbf{\Phi}$ que satisfaz
    o D-RIP de ordem $s$.
    Então, a solução $\vec{f'}$ para a análise $\ell_1$ satisfaz:
    \begin{equation*}
        \lVert \vec{f'} - \vec{f} \rVert_2 \le C \left[ \epsilon + \frac{\lVert \mathbf{D}^* \vec{f} - \left( \mathbf{D}^* \vec{f} \right)_s}{\sqrt{s}} \right],
    \end{equation*}
    tal que $\left( \mathbf{D}^* \vec{f} \right)_s$ denota o vetor com os $s$ maiores coeficientes em 
    magnitude de $\mathbf{D}^* \vec{f}$ e $C$ é uma constante tal que $C \ge 0$.
\end{teorema}

Esse novo campo de estudo abriu caminhos para novas análises e usos de CS para outros
problemas, antes não possível, além tornar possível utilizar os modelos e técnicas de CS
para outros campos do conhecimento.

% \subsubsection{Métodos utilizando \textit{Total Variation}}
% Decidir se terá ou não

\section{Aprendizado de Dicionários}
A área de aprendizado de dicionários, do inglês \textit{Dictionary Learning}, busca, através
dos dados já conhecidos de uma classe de sinais, prover novos dicionários nos quais os sinais 
serão capazes de se conformar com maior esparsidade. 
Sob o ponto de vista de CS, um dicionário $\mat{D}$ normalmente é escolhido de forma aleatória,
desde que satisfaça a RIP.
Aprendizado de dicionários, por outro lado, busca gerar modelos considerando os sinais envolvidos,
ou seja, dicionários que dependem dos dados, ou \textit{data-dependent dictionaries} \cite{chen2015compressed}.

Nas seções seguintes será explicitado o problema geral de aprendizado de dicionários, bem como 
um algoritmo que é capaz de de gerar um dicionário a partir de dados observados, e, por fim, 
uma aplicação focada em processamento de imagens.

\subsection{Problema Geral de Aprendizado de Dicionários}
Nessa seção será apresentado um modelo geral do problema que aprendizado de dicionários busca resolver.
De forma intuitiva, pode-se imaginar aprendizado de dicionários como se fosse desejado gerar 
um sub-dicionário da língua portuguesa a partir de uma grande quantidade textos, livros, todos 
eles separados por palavras.

Mais formalmente, suponha-se $\vec{x_1}, \vec{x_2}, ..., \vec{x_n} \in \mathbb{R}^L$
um conjunto finito de sinais de trainamento, e inteiros positivos $m, s$.
Deseja-se encontrar a matriz $\mat{D}$ e os s-esparsos vetores 
$\vec{\gamma_1}, \vec{\gamma_2}, ..., \vec{\gamma_n} \in \mathbb{R}^m$ os quais
$\mat{D}\vec{\gamma_i} \approx x_i$ para todo $i$. Dessa forma, pode-se escrever o problema
de aprendizado de dicionários utilizando a norma $\ell_2$ como segue  \cite{chen2015compressed}:
\begin{equation}
    \label{eq:dlgeneral}
    \underset{\mat{D}, \vec{\gamma_1}...\vec{\gamma_n}}{min} 
    \sum_{i=1}^{n} {\lVert  \vec{x_i} - \mat{D} \vec{\gamma_i} \rVert \SPSB{2}{2}}
    \hspace{1em} \text{tal que} \hspace{1em}
    \lVert \gamma_i \rVert_0 \le s, \text{ para todo } i.
\end{equation}

Nesse caso, $\mat{D} \in \mathbb{R}^{L\times m}$ é o dicionário aprendido, enquanto os 
vetores $\gamma_i \in \mathbb{R}^m$, com no máximo $s$ coeficientes não nulos, é a Representação
linear de $\vec{x_i}$ em $\mat{D}$. O valor de $m$ pode ser maior que o de $L$, talvez para 
explorar alguma redundância, mas $s\ll L$. Outra coisa que fica clara é que, para que as escolhas
de $\gamma_i$ e $\mat{D}$ sejam únicas, pode-se forçar que as colunas de $\mat{D}$ tenham norma $\ell_2$
unitária \cite{chen2015compressed}.



\subsection{\textit{Online Dictionary Learning}}

\subsection{Codificação Esparsa}

% \section{Codificação de Imagens}
% Decidir se terá ou não

\section{Codificação de Vídeo}

\chapter{Trabalhos Relacionados}

\chapter{Proposta}

\chapter{Resultados}

\chapter{Conclusão}

% referências
% aqui será usado o environment padrao `thebibliography'; porém, sugere-se
% seriamente o uso de BibTeX e do estilo abnt.bst (veja na página do
% UTUG)
% 
% observe também o estilo meio estranho de alguns labels; isso é
% devido ao uso do pacote `natbib', que permite fazer citações de
% autores, ano, e diversas combinações desses

\bibliographystyle{abntex2-alf}
\bibliography{biblio}

\end{document}
